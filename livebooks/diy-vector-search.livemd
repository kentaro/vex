<!-- livebook:{"app_settings":{"slug":"antipop"}} -->

# How to create your own vector search engine in Elixir

```elixir
Mix.install(
  [
    {:bumblebee, "~> 0.3.0"},
    {:exla, ">= 0.0.0"},
    {:flow, "~> 1.2.4"},
    {:req, "~> 0.3"},
    {:jumanpp, "~> 0.1.0"},
    {:ex_faiss, github: "elixir-nx/ex_faiss"}
  ],
  config: [nx: [default_backend: EXLA.Backend]],
  system_env: %{"USE_LLVM_BREW" => "true"}
)
```

## Introduction

Along with the emerging LLMs, vector search engines have become increasingly important nowadays. We will demonstrate how to create a DIY vector search engine to gain detailed knowledge about the technology.

## Dataset

We use JSQuAD distributed in [yahoojapan/JGLUE: JGLUE: Japanese General Language Understanding Evaluation](https://github.com/yahoojapan/JGLUE) to demonstrate our DIY vector search engine.

```elixir
dataset =
  "https://raw.githubusercontent.com/yahoojapan/JGLUE/main/datasets/jsquad-v1.1/train-v1.1.json"
  |> Req.get!()
  |> Map.get(:body)
  |> Jason.decode!()
  |> Map.get("data")
```

```elixir
docs =
  dataset
  |> Enum.map(fn data ->
    data
    |> Map.get("paragraphs")
    |> Enum.map(fn paragraph ->
      paragraph |> Map.get("context")
    end)
  end)
  |> List.flatten()
```

The code below takes a long time, because `Jumanpp.parse!` calls the `jumanpp` command in a shell context.

```elixir
preprocessed =
  docs
  |> Flow.from_enumerable(stages: 10)
  |> Flow.map(fn doc ->
    doc
    |> String.replace(~r/^.+ \[SEP\] /, "")
    |> Jumanpp.parse!()
    |> Enum.map(fn token ->
      token |> Map.get(:surface)
    end)
    |> Enum.join(" ")
  end)
  |> Enum.to_list()
```

## Tokenizer

We use [ku-nlp/deberta-v2-base-japanese](https://huggingface.co/ku-nlp/deberta-v2-base-japanese) for a tokenizer and a linguistic model for embedding.

Since Bumblebee currently doesn't support DeBERTa, we use `Bumblebee.Text.BartTokenizer` as a related module instead. It's hackish, though.

```elixir
{:ok, tokenizer} =
  Bumblebee.load_tokenizer(
    {:hf, "ku-nlp/deberta-v2-base-japanese"},
    module: Bumblebee.Text.BartTokenizer
  )
```

```elixir
inputs = Bumblebee.apply_tokenizer(tokenizer, preprocessed)
```

## Embedding

```elixir
{:ok, %{model: model, params: params, spec: spec}} =
  Bumblebee.load_model(
    {:hf, "ku-nlp/deberta-v2-base-japanese"},
    module: Bumblebee.Text.Bart
  )
```

```elixir
outputs = Axon.predict(model, params, inputs)
```

In the code below, we calcurate means for columns of hidden states, which represent sentence vectors.

```elixir
outputs.hidden_state[0] |> Nx.mean(axes: [0])
```

## Vector Store

We use [ex_faiss](https://github.com/elixir-nx/ex_faiss) which is a wrapper library of Faiss similarity search engine.

```elixir
index = ExFaiss.Index.new(1024, "Flat")
```

```elixir
{count, _, _} = outputs.hidden_state |> Nx.shape()

index =
  0..(count - 1)
  |> Enum.reduce(index, fn i, acc ->
    acc |> ExFaiss.Index.add(outputs.hidden_state[i] |> Nx.mean(axes: [0]))
  end)
```

In the code below, we send a query to the vector search engine. The query is converted into an embedding using the same method above.

```elixir
# query = "今日 は いい 天気 。"
query = "映画 『 戦場 の メリークリスマス 』 で 映画 に 俳優 と して 出演 し た 音楽家 は 誰 ？"

query_input =
  tokenizer
  |> Bumblebee.apply_tokenizer(query)

query_output = Axon.predict(model, params, query_input)
query_vector = query_output.hidden_state[0] |> Nx.mean(axes: [0])
```

```elixir
doc_ids =
  index
  |> ExFaiss.Index.search(query_vector, 5)
  |> Map.get(:labels)
  |> Nx.to_list()
  |> List.flatten()
```

```elixir
doc_ids
|> Enum.map(fn i ->
  docs |> Enum.at(i)
end)
```
